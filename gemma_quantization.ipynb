{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be downloaded to: ./gemma2b-nirf-lookup\n",
      "GGUF files will be saved to: ./gguf_output\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"coderop12/gemma2b-nirf-lookup-2025\"\n",
    "LOCAL_MODEL_DIR = \"./gemma2b-nirf-lookup\"\n",
    "GGUF_OUTPUT_DIR = \"./gguf_output\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(GGUF_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Model will be downloaded to: {LOCAL_MODEL_DIR}\")\n",
    "print(f\"GGUF files will be saved to: {GGUF_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668f14a850b842e1996a4ffdd181ed13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model downloaded successfully!\n",
      "\n",
      "Downloaded files:\n",
      "  - .gitattributes\n",
      "  - special_tokens_map.json\n",
      "  - generation_config.json\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - tokenizer_config.json\n",
      "  - model-00002-of-00002.safetensors\n",
      "  - chat_template.jinja\n",
      "  - config.json\n",
      "  - README.md\n",
      "  - tokenizer.json\n",
      "  - NOTICE\n",
      "  - .cache\n",
      "  - model.safetensors.index.json\n",
      "  - tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "# Download the model from Hugging Face\n",
    "print(\"Downloading model from Hugging Face...\")\n",
    "try:\n",
    "    snapshot_download(\n",
    "        repo_id=MODEL_NAME,\n",
    "        local_dir=LOCAL_MODEL_DIR,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    print(\"✅ Model downloaded successfully!\")\n",
    "    \n",
    "    # List downloaded files\n",
    "    print(\"\\nDownloaded files:\")\n",
    "    for file in os.listdir(LOCAL_MODEL_DIR):\n",
    "        print(f\"  - {file}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking llama.cpp installation...\n",
      "❌ llama-cpp-python not found - you may need to install it\n",
      "❌ Conversion script not found - you may need to clone llama.cpp repo\n"
     ]
    }
   ],
   "source": [
    "# Check if llama.cpp is available\n",
    "def check_llama_cpp():\n",
    "    try:\n",
    "        result = subprocess.run(['python', '-c', 'import llama_cpp'], \n",
    "                              capture_output=True, text=True)\n",
    "        return result.returncode == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_conversion_script():\n",
    "    # Common paths where convert script might be\n",
    "    possible_paths = [\n",
    "        \"convert-hf-to-gguf.py\",\n",
    "        \"./llama.cpp/convert-hf-to-gguf.py\",\n",
    "        \"convert_hf_to_gguf.py\"\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "print(\"Checking llama.cpp installation...\")\n",
    "if check_llama_cpp():\n",
    "    print(\"✅ llama-cpp-python found\")\n",
    "else:\n",
    "    print(\"❌ llama-cpp-python not found - you may need to install it\")\n",
    "\n",
    "convert_script = check_conversion_script()\n",
    "if convert_script:\n",
    "    print(f\"✅ Conversion script found: {convert_script}\")\n",
    "    CONVERT_SCRIPT = convert_script\n",
    "else:\n",
    "    print(\"❌ Conversion script not found - you may need to clone llama.cpp repo\")\n",
    "    CONVERT_SCRIPT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing llama-cpp-python...\n",
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "❌ Failed to install llama-cpp-python: Command '['/home/zeus/miniconda3/envs/cloudspace/bin/python', '-m', 'pip', 'install', 'llama-cpp-python']' returned non-zero exit status 1.\n",
      "\n",
      "Cloning llama.cpp repository...\n",
      "Cloning into 'llama.cpp'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[171 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[32m*** \u001b[1mscikit-build-core 0.11.6\u001b[0m using \u001b[34mCMake 3.28.3\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m loading initial cache file /tmp/tmpuvi2lzk4/build/CMakeInit.txt\n",
      "  \u001b[31m   \u001b[0m -- The C compiler identification is GNU 13.3.0\n",
      "  \u001b[31m   \u001b[0m -- The CXX compiler identification is GNU 13.3.0\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working C compiler: /usr/bin/gcc - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
      "  \u001b[31m   \u001b[0m \u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
      "  \u001b[31m   \u001b[0m -- Found Git: /commands/git (found version \"2.42.0\")\n",
      "  \u001b[31m   \u001b[0m -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  \u001b[31m   \u001b[0m -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  \u001b[31m   \u001b[0m -- Found Threads: TRUE\n",
      "  \u001b[31m   \u001b[0m -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "  \u001b[31m   \u001b[0m -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  \u001b[31m   \u001b[0m -- GGML_SYSTEM_ARCH: x86\n",
      "  \u001b[31m   \u001b[0m -- Including CPU backend\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP: TRUE (found version \"4.5\")\n",
      "  \u001b[31m   \u001b[0m -- x86 detected\n",
      "  \u001b[31m   \u001b[0m -- Adding CPU backend variant ggml-cpu: -march=native\n",
      "  \u001b[31m   \u001b[0m -- ggml version: 0.0.1\n",
      "  \u001b[31m   \u001b[0m -- ggml commit:  4227c9b\n",
      "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "  \u001b[31m   \u001b[0m   Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  \u001b[31m   \u001b[0m Call Stack (most recent call first):\n",
      "  \u001b[31m   \u001b[0m   CMakeLists.txt:108 (llama_cpp_python_install_target)\n",
      "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "  \u001b[31m   \u001b[0m   Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  \u001b[31m   \u001b[0m Call Stack (most recent call first):\n",
      "  \u001b[31m   \u001b[0m   CMakeLists.txt:108 (llama_cpp_python_install_target)\n",
      "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "  \u001b[31m   \u001b[0m   Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  \u001b[31m   \u001b[0m Call Stack (most recent call first):\n",
      "  \u001b[31m   \u001b[0m   CMakeLists.txt:109 (llama_cpp_python_install_target)\n",
      "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "  \u001b[31m   \u001b[0m   Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  \u001b[31m   \u001b[0m Call Stack (most recent call first):\n",
      "  \u001b[31m   \u001b[0m   CMakeLists.txt:109 (llama_cpp_python_install_target)\n",
      "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "  \u001b[31m   \u001b[0m   Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  \u001b[31m   \u001b[0m Call Stack (most recent call first):\n",
      "  \u001b[31m   \u001b[0m   CMakeLists.txt:162 (llama_cpp_python_install_target)\n",
      "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "  \u001b[31m   \u001b[0m   Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  \u001b[31m   \u001b[0m Call Stack (most recent call first):\n",
      "  \u001b[31m   \u001b[0m   CMakeLists.txt:162 (llama_cpp_python_install_target)\n",
      "  \u001b[31m   \u001b[0m This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m -- Configuring done (1.0s)\n",
      "  \u001b[31m   \u001b[0m -- Generating done (0.0s)\n",
      "  \u001b[31m   \u001b[0m -- Build files have been written to: /tmp/tmpuvi2lzk4/build\n",
      "  \u001b[31m   \u001b[0m \u001b[32m***\u001b[0m \u001b[1mBuilding project with \u001b[34mNinja\u001b[39m...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Change Dir: '/tmp/tmpuvi2lzk4/build'\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Run Build Command(s): ninja -v\n",
      "  \u001b[31m   \u001b[0m [1/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-threading.cpp\n",
      "  \u001b[31m   \u001b[0m [2/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml.cpp\n",
      "  \u001b[31m   \u001b[0m [3/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/hbm.cpp\n",
      "  \u001b[31m   \u001b[0m [4/84] /usr/bin/gcc  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-alloc.c\n",
      "  \u001b[31m   \u001b[0m [5/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp\n",
      "  \u001b[31m   \u001b[0m [6/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/traits.cpp\n",
      "  \u001b[31m   \u001b[0m [7/84] /usr/bin/gcc  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c\n",
      "  \u001b[31m   \u001b[0m [8/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-backend.cpp\n",
      "  \u001b[31m   \u001b[0m [9/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-opt.cpp\n",
      "  \u001b[31m   \u001b[0m [10/84] /usr/bin/gcc  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/quants.c\n",
      "  \u001b[31m   \u001b[0m [11/84] /usr/bin/gcc  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml.c\n",
      "  \u001b[31m   \u001b[0m [12/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp\n",
      "  \u001b[31m   \u001b[0m [13/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp\n",
      "  \u001b[31m   \u001b[0m [14/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/vec.cpp\n",
      "  \u001b[31m   \u001b[0m [15/84] /usr/bin/gcc  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/quants.c\n",
      "  \u001b[31m   \u001b[0m [16/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp\n",
      "  \u001b[31m   \u001b[0m [17/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-backend-reg.cpp\n",
      "  \u001b[31m   \u001b[0m [18/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp\n",
      "  \u001b[31m   \u001b[0m [19/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp\n",
      "  \u001b[31m   \u001b[0m [20/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/gguf.cpp\n",
      "  \u001b[31m   \u001b[0m [21/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama.cpp\n",
      "  \u001b[31m   \u001b[0m [22/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-cparams.cpp\n",
      "  \u001b[31m   \u001b[0m [23/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-adapter.cpp\n",
      "  \u001b[31m   \u001b[0m [24/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-batch.cpp\n",
      "  \u001b[31m   \u001b[0m [25/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-arch.cpp\n",
      "  \u001b[31m   \u001b[0m [26/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-chat.cpp\n",
      "  \u001b[31m   \u001b[0m [27/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-hparams.cpp\n",
      "  \u001b[31m   \u001b[0m [28/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-io.cpp\n",
      "  \u001b[31m   \u001b[0m [29/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-graph.cpp\n",
      "  \u001b[31m   \u001b[0m [30/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-impl.cpp\n",
      "  \u001b[31m   \u001b[0m [31/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-context.cpp\n",
      "  \u001b[31m   \u001b[0m [32/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-memory.cpp\n",
      "  \u001b[31m   \u001b[0m [33/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/repack.cpp\n",
      "  \u001b[31m   \u001b[0m [34/84] /usr/bin/gcc  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-quants.c\n",
      "  \u001b[31m   \u001b[0m [35/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml-base.so -o bin/libggml-base.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  -lm && :\n",
      "  \u001b[31m   \u001b[0m [36/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp\n",
      "  \u001b[31m   \u001b[0m [37/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-mmap.cpp\n",
      "  \u001b[31m   \u001b[0m [38/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-kv-cache-unified-iswa.cpp\n",
      "  \u001b[31m   \u001b[0m [39/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-model-saver.cpp\n",
      "  \u001b[31m   \u001b[0m [40/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-memory-hybrid.cpp\n",
      "  \u001b[31m   \u001b[0m [41/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-memory-recurrent.cpp\n",
      "  \u001b[31m   \u001b[0m [42/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/unicode-data.cpp\n",
      "  \u001b[31m   \u001b[0m [43/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat   -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/tmpuvi2lzk4/build/vendor/llama.cpp/common/build-info.cpp\n",
      "  \u001b[31m   \u001b[0m [44/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/ggml-cpu/ops.cpp\n",
      "  \u001b[31m   \u001b[0m [45/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml-cpu.so -o bin/libggml-cpu.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  bin/libggml-base.so  /usr/lib/gcc/x86_64-linux-gnu/13/libgomp.so && :\n",
      "  \u001b[31m   \u001b[0m [46/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml.so -o bin/libggml.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  -ldl  bin/libggml-cpu.so  bin/libggml-base.so && :\n",
      "  \u001b[31m   \u001b[0m [47/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-kv-cache-unified.cpp\n",
      "  \u001b[31m   \u001b[0m [48/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-grammar.cpp\n",
      "  \u001b[31m   \u001b[0m [49/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-model-loader.cpp\n",
      "  \u001b[31m   \u001b[0m [50/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/console.cpp\n",
      "  \u001b[31m   \u001b[0m [51/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-vocab.cpp\n",
      "  \u001b[31m   \u001b[0m [52/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-quant.cpp\n",
      "  \u001b[31m   \u001b[0m [53/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/chat-parser.cpp\n",
      "  \u001b[31m   \u001b[0m [54/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/llguidance.cpp\n",
      "  \u001b[31m   \u001b[0m [55/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/log.cpp\n",
      "  \u001b[31m   \u001b[0m [56/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-sampling.cpp\n",
      "  \u001b[31m   \u001b[0m [57/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/ngram-cache.cpp\n",
      "  \u001b[31m   \u001b[0m [58/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/json-partial.cpp\n",
      "  \u001b[31m   \u001b[0m [59/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/speculative.cpp\n",
      "  \u001b[31m   \u001b[0m [60/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/sampling.cpp\n",
      "  \u001b[31m   \u001b[0m [61/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/common.cpp\n",
      "  \u001b[31m   \u001b[0m [62/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/unicode.cpp\n",
      "  \u001b[31m   \u001b[0m [63/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp\n",
      "  \u001b[31m   \u001b[0m [64/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp\n",
      "  \u001b[31m   \u001b[0m [65/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/mtmd.cpp\n",
      "  \u001b[31m   \u001b[0m [66/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp\n",
      "  \u001b[31m   \u001b[0m [67/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/mtmd-audio.cpp\n",
      "  \u001b[31m   \u001b[0m [68/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp\n",
      "  \u001b[31m   \u001b[0m [69/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-llava-cli   && :\n",
      "  \u001b[31m   \u001b[0m [70/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-gemma3-cli   && :\n",
      "  \u001b[31m   \u001b[0m [71/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-minicpmv-cli   && :\n",
      "  \u001b[31m   \u001b[0m [72/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-qwen2vl-cli   && :\n",
      "  \u001b[31m   \u001b[0m [73/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/regex-partial.cpp\n",
      "  \u001b[31m   \u001b[0m [74/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/include -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/mtmd-cli.cpp\n",
      "  \u001b[31m   \u001b[0m [75/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/clip.cpp\n",
      "  \u001b[31m   \u001b[0m [76/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/json-schema-to-grammar.cpp\n",
      "  \u001b[31m   \u001b[0m [77/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/arg.cpp\n",
      "  \u001b[31m   \u001b[0m [78/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/llama-model.cpp\n",
      "  \u001b[31m   \u001b[0m [79/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o bin/libllama.so vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-base.so && :\n",
      "  \u001b[31m   \u001b[0m [80/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/tools/mtmd/mtmd-helper.cpp\n",
      "  \u001b[31m   \u001b[0m [81/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libmtmd.so -o vendor/llama.cpp/tools/mtmd/libmtmd.so vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-base.so && :\n",
      "  \u001b[31m   \u001b[0m [82/84] /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DLLAMA_SHARED -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/. -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/src/../include -I/tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o -c /tmp/pip-install-4yrt3ght/llama-cpp-python_a8149dbc85c9446f9b8f6818dfcdb1f7/vendor/llama.cpp/common/chat.cpp\n",
      "  \u001b[31m   \u001b[0m [83/84] : && /usr/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n",
      "  \u001b[31m   \u001b[0m [84/84] : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-mtmd-cli  -Wl,-rpath,/tmp/tmpuvi2lzk4/build/vendor/llama.cpp/tools/mtmd:/tmp/tmpuvi2lzk4/build/bin:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/tools/mtmd/libmtmd.so  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-base.so && :\n",
      "  \u001b[31m   \u001b[0m \u001b[31mFAILED: [code=1] \u001b[0mvendor/llama.cpp/tools/mtmd/llama-mtmd-cli\n",
      "  \u001b[31m   \u001b[0m : && /usr/bin/g++  -pthread -B /home/zeus/miniconda3/envs/cloudspace/compiler_compat -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-mtmd-cli  -Wl,-rpath,/tmp/tmpuvi2lzk4/build/vendor/llama.cpp/tools/mtmd:/tmp/tmpuvi2lzk4/build/bin:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/tools/mtmd/libmtmd.so  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-base.so && :\n",
      "  \u001b[31m   \u001b[0m /home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: warning: libgomp.so.1, needed by bin/libggml-cpu.so, not found (try using -rpath or -rpath-link)\n",
      "  \u001b[31m   \u001b[0m /home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: bin/libggml-cpu.so: undefined reference to `GOMP_barrier@GOMP_1.0'\n",
      "  \u001b[31m   \u001b[0m /home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: bin/libggml-cpu.so: undefined reference to `GOMP_parallel@GOMP_4.0'\n",
      "  \u001b[31m   \u001b[0m /home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: bin/libggml-cpu.so: undefined reference to `omp_get_thread_num@OMP_1.0'\n",
      "  \u001b[31m   \u001b[0m /home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: bin/libggml-cpu.so: undefined reference to `GOMP_single_start@GOMP_1.0'\n",
      "  \u001b[31m   \u001b[0m /home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: bin/libggml-cpu.so: undefined reference to `omp_get_num_threads@OMP_1.0'\n",
      "  \u001b[31m   \u001b[0m collect2: error: ld returned 1 exit status\n",
      "  \u001b[31m   \u001b[0m ninja: build stopped: subcommand failed.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m\n",
      "  \u001b[31m   \u001b[0m \u001b[1m***\u001b[0m \u001b[31mCMake build failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
      "\u001b[31m╰─>\u001b[0m llama-cpp-python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ llama.cpp repository cloned successfully!\n",
      "Conversion script available at: ./llama.cpp/convert_hf_to_gguf.py\n"
     ]
    }
   ],
   "source": [
    "# Install llama-cpp-python and clone llama.cpp repo\n",
    "print(\"Installing llama-cpp-python...\")\n",
    "try:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'llama-cpp-python'], \n",
    "                   check=True)\n",
    "    print(\"✅ llama-cpp-python installed successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Failed to install llama-cpp-python: {e}\")\n",
    "\n",
    "print(\"\\nCloning llama.cpp repository...\")\n",
    "try:\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/ggerganov/llama.cpp.git'], \n",
    "                   check=True)\n",
    "    print(\"✅ llama.cpp repository cloned successfully!\")\n",
    "    \n",
    "    # Set the conversion script path\n",
    "    CONVERT_SCRIPT = \"./llama.cpp/convert_hf_to_gguf.py\"\n",
    "    print(f\"Conversion script available at: {CONVERT_SCRIPT}\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Failed to clone llama.cpp: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying alternative installation methods...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.3.16)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "✅ Pre-built llama-cpp-python installed successfully!\n",
      "✅ Conversion script ready: ./llama.cpp/convert_hf_to_gguf.py\n"
     ]
    }
   ],
   "source": [
    "# Try installing a pre-built version or use alternative method\n",
    "print(\"Trying alternative installation methods...\")\n",
    "\n",
    "# First, try installing without building from source\n",
    "try:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', \n",
    "                   'llama-cpp-python', '--only-binary=all'], \n",
    "                   check=True)\n",
    "    print(\"✅ Pre-built llama-cpp-python installed successfully!\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"❌ Pre-built version failed, trying with environment variables...\")\n",
    "    \n",
    "    # Try with specific environment variables to fix OpenMP issues\n",
    "    env = os.environ.copy()\n",
    "    env['CMAKE_ARGS'] = '-DGGML_OPENMP=OFF'\n",
    "    \n",
    "    try:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'llama-cpp-python'], \n",
    "                       env=env, check=True)\n",
    "        print(\"✅ llama-cpp-python installed without OpenMP!\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"❌ Still failing, let's proceed with just the conversion script\")\n",
    "\n",
    "# Verify we have the conversion script\n",
    "CONVERT_SCRIPT = \"./llama.cpp/convert_hf_to_gguf.py\"\n",
    "if os.path.exists(CONVERT_SCRIPT):\n",
    "    print(f\"✅ Conversion script ready: {CONVERT_SCRIPT}\")\n",
    "else:\n",
    "    print(\"❌ Conversion script not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to get compatible tokenizer...\n",
      "Downloading original Gemma 2B tokenizer.model...\n",
      "✅ Copied tokenizer.model to ./gemma2b-nirf-lookup/tokenizer.model\n",
      "Retrying conversion with SentencePiece tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conversion completed successfully!\n",
      "STDOUT: \n"
     ]
    }
   ],
   "source": [
    "# Try to download the original Gemma 2B tokenizer.model file\n",
    "print(\"Attempting to get compatible tokenizer...\")\n",
    "\n",
    "try:\n",
    "    # Download the base Gemma 2B model's tokenizer\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    \n",
    "    print(\"Downloading original Gemma 2B tokenizer.model...\")\n",
    "    tokenizer_model_path = hf_hub_download(\n",
    "        repo_id=\"google/gemma-2b\", \n",
    "        filename=\"tokenizer.model\",\n",
    "        local_dir=\"./gemma_base_tokenizer\"\n",
    "    )\n",
    "    \n",
    "    # Copy it to our model directory\n",
    "    import shutil\n",
    "    dest_path = os.path.join(LOCAL_MODEL_DIR, \"tokenizer.model\")\n",
    "    shutil.copy2(tokenizer_model_path, dest_path)\n",
    "    \n",
    "    print(f\"✅ Copied tokenizer.model to {dest_path}\")\n",
    "    \n",
    "    # Now try the conversion again\n",
    "    print(\"Retrying conversion with SentencePiece tokenizer...\")\n",
    "    \n",
    "    output_file = os.path.join(GGUF_OUTPUT_DIR, \"gemma2b-nirf-lookup-f16.gguf\")\n",
    "    cmd = [\n",
    "        sys.executable, \n",
    "        CONVERT_SCRIPT,\n",
    "        LOCAL_MODEL_DIR,\n",
    "        \"--outfile\", output_file,\n",
    "        \"--outtype\", \"f16\"\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"✅ Conversion completed successfully!\")\n",
    "    print(\"STDOUT:\", result.stdout[-500:])  # Show last 500 chars\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Let's try an alternative approach...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking conversion results...\n",
      "\n",
      "Files in ./gguf_output:\n",
      "  - gemma2b-nirf-lookup-f16.gguf (4992.7 MB)\n",
      "\n",
      "F16 conversion complete! Would you like to create quantized versions?\n",
      "Available quantization options:\n",
      "  - Q4_0: ~2.3GB, good balance of size/quality\n",
      "  - Q4_1: ~2.6GB, slightly better quality\n",
      "  - Q5_0: ~2.9GB, better quality\n",
      "  - Q5_1: ~3.2GB, even better quality\n",
      "  - Q8_0: ~4.3GB, minimal quality loss\n",
      "\n",
      "Creating Q4_0 quantized version...\n"
     ]
    }
   ],
   "source": [
    "# Check the conversion results\n",
    "print(\"Checking conversion results...\")\n",
    "\n",
    "# List all files in output directory\n",
    "print(f\"\\nFiles in {GGUF_OUTPUT_DIR}:\")\n",
    "for file in os.listdir(GGUF_OUTPUT_DIR):\n",
    "    file_path = os.path.join(GGUF_OUTPUT_DIR, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"  - {file} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Now let's check if we want to create quantized versions\n",
    "print(\"\\nF16 conversion complete! Would you like to create quantized versions?\")\n",
    "print(\"Available quantization options:\")\n",
    "print(\"  - Q4_0: ~2.3GB, good balance of size/quality\")\n",
    "print(\"  - Q4_1: ~2.6GB, slightly better quality\") \n",
    "print(\"  - Q5_0: ~2.9GB, better quality\")\n",
    "print(\"  - Q5_1: ~3.2GB, even better quality\")\n",
    "print(\"  - Q8_0: ~4.3GB, minimal quality loss\")\n",
    "\n",
    "# Let's create a popular Q4_0 version as an example\n",
    "print(\"\\nCreating Q4_0 quantized version...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying build with CURL disabled...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cmake...\n",
      "Building quantize tool...\n",
      "✅ Build successful!\n",
      "Running quantization: ./llama.cpp/build/llama-quantize ./gguf_output/gemma2b-nirf-lookup-f16.gguf ./gguf_output/gemma2b-nirf-lookup-q4_0.gguf Q4_0\n",
      "Build/quantization process encountered issues: [Errno 2] No such file or directory: './llama.cpp/build/llama-quantize'\n",
      "\n",
      "✅ However, your F16 GGUF conversion was successful!\n",
      "You can use the F16 version or manually quantize it later.\n",
      "\n",
      "📁 Final files in ./gguf_output:\n",
      "  - gemma2b-nirf-lookup-f16.gguf (4992.7 MB)\n",
      "\n",
      "✅ GGUF conversion completed successfully!\n",
      "Your model is ready: ./gguf_output/gemma2b-nirf-lookup-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Try building with CURL disabled\n",
    "print(\"Retrying build with CURL disabled...\")\n",
    "\n",
    "try:\n",
    "    # Clean build directory first\n",
    "    import shutil\n",
    "    if os.path.exists(\"./llama.cpp/build\"):\n",
    "        shutil.rmtree(\"./llama.cpp/build\")\n",
    "    \n",
    "    os.makedirs(\"./llama.cpp/build\", exist_ok=True)\n",
    "    \n",
    "    # Build with CURL disabled\n",
    "    cmake_cmd = [\n",
    "        \"cmake\", \"-B\", \"./llama.cpp/build\", \"-S\", \"./llama.cpp\",\n",
    "        \"-DLLAMA_CURL=OFF\",\n",
    "        \"-DGGML_OPENMP=OFF\"  # Also disable OpenMP to avoid previous issues\n",
    "    ]\n",
    "    \n",
    "    print(\"Running cmake...\")\n",
    "    cmake_result = subprocess.run(cmake_cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if cmake_result.returncode != 0:\n",
    "        print(\"CMake failed:\", cmake_result.stderr)\n",
    "        raise Exception(\"CMake configuration failed\")\n",
    "    \n",
    "    # Build just the quantize tool\n",
    "    make_cmd = [\"make\", \"-C\", \"./llama.cpp/build\", \"-j4\", \"llama-quantize\"]\n",
    "    print(\"Building quantize tool...\")\n",
    "    \n",
    "    make_result = subprocess.run(make_cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if make_result.returncode != 0:\n",
    "        print(\"Build failed:\", make_result.stderr)\n",
    "        # Try alternative: use Python quantization\n",
    "        print(\"Build failed, using Python-based quantization instead...\")\n",
    "        \n",
    "        # Use llama-cpp-python for quantization\n",
    "        from llama_cpp import Llama\n",
    "        \n",
    "        input_file = os.path.join(GGUF_OUTPUT_DIR, \"gemma2b-nirf-lookup-f16.gguf\")\n",
    "        output_file = os.path.join(GGUF_OUTPUT_DIR, \"gemma2b-nirf-lookup-q4_0.gguf\")\n",
    "        \n",
    "        print(\"Loading model for quantization...\")\n",
    "        # Note: This approach loads the model to quantize it\n",
    "        # For very large models, this might use significant RAM\n",
    "        llm = Llama(model_path=input_file, verbose=False)\n",
    "        \n",
    "        print(\"Quantizing to Q4_0...\")\n",
    "        # This is a conceptual approach - actual quantization with llama-cpp-python \n",
    "        # requires different methods\n",
    "        print(\"❌ Direct quantization via Python not easily available\")\n",
    "        print(\"The F16 GGUF file is ready for use though!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"✅ Build successful!\")\n",
    "        \n",
    "        # Now try quantization\n",
    "        quantize_tool = \"./llama.cpp/build/llama-quantize\"\n",
    "        input_file = os.path.join(GGUF_OUTPUT_DIR, \"gemma2b-nirf-lookup-f16.gguf\")\n",
    "        output_file = os.path.join(GGUF_OUTPUT_DIR, \"gemma2b-nirf-lookup-q4_0.gguf\")\n",
    "        \n",
    "        quant_cmd = [quantize_tool, input_file, output_file, \"Q4_0\"]\n",
    "        print(f\"Running quantization: {' '.join(quant_cmd)}\")\n",
    "        \n",
    "        result = subprocess.run(quant_cmd, capture_output=True, text=True, check=True)\n",
    "        print(\"✅ Q4_0 quantization completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Build/quantization process encountered issues: {e}\")\n",
    "    print(\"\\n✅ However, your F16 GGUF conversion was successful!\")\n",
    "    print(\"You can use the F16 version or manually quantize it later.\")\n",
    "\n",
    "# Show final status\n",
    "print(f\"\\n📁 Final files in {GGUF_OUTPUT_DIR}:\")\n",
    "for file in os.listdir(GGUF_OUTPUT_DIR):\n",
    "    file_path = os.path.join(GGUF_OUTPUT_DIR, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"  - {file} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\n✅ GGUF conversion completed successfully!\")\n",
    "print(f\"Your model is ready: {os.path.join(GGUF_OUTPUT_DIR, 'gemma2b-nirf-lookup-f16.gguf')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GGUF CONVERSION SUMMARY ===\n",
      "\n",
      "✅ Successfully converted: coderop12/gemma2b-nirf-lookup-2025\n",
      "📁 Output file: ./gguf_output/gemma2b-nirf-lookup-f16.gguf\n",
      "📊 File size: 4.88 GB\n",
      "🔧 Format: GGUF F16\n",
      "✅ File size appears reasonable for a 2B parameter model\n",
      "\n",
      "📋 Model ready for use with:\n",
      "   - llama.cpp\n",
      "   - Ollama\n",
      "   - text-generation-webui\n",
      "   - LM Studio\n",
      "   - Any GGUF-compatible inference engine\n",
      "\n",
      "🎯 CONVERSION COMPLETE!\n",
      "The specialized Gemma 2B NIRF lookup model is now ready in GGUF format.\n",
      "\n",
      "💡 Usage example:\n",
      "   llama.cpp: ./llama-cli -m ./gguf_output/gemma2b-nirf-lookup-f16.gguf -p 'Your prompt here'\n",
      "   Python: llama = Llama(model_path='./gguf_output/gemma2b-nirf-lookup-f16.gguf')\n"
     ]
    }
   ],
   "source": [
    "# Final verification and summary\n",
    "print(\"=== GGUF CONVERSION SUMMARY ===\\n\")\n",
    "\n",
    "# Verify the final file\n",
    "gguf_file = os.path.join(GGUF_OUTPUT_DIR, \"gemma2b-nirf-lookup-f16.gguf\")\n",
    "if os.path.exists(gguf_file):\n",
    "    file_size_gb = os.path.getsize(gguf_file) / (1024**3)\n",
    "    print(f\"✅ Successfully converted: {MODEL_NAME}\")\n",
    "    print(f\"📁 Output file: {gguf_file}\")\n",
    "    print(f\"📊 File size: {file_size_gb:.2f} GB\")\n",
    "    print(f\"🔧 Format: GGUF F16\")\n",
    "    \n",
    "    # Basic file validation\n",
    "    try:\n",
    "        # Check if file is readable and has reasonable size\n",
    "        if file_size_gb > 1.0 and file_size_gb < 10.0:  # Reasonable range for 2B model\n",
    "            print(\"✅ File size appears reasonable for a 2B parameter model\")\n",
    "        else:\n",
    "            print(\"⚠️  File size seems unusual - please verify\")\n",
    "            \n",
    "        print(f\"\\n📋 Model ready for use with:\")\n",
    "        print(f\"   - llama.cpp\")\n",
    "        print(f\"   - Ollama\")\n",
    "        print(f\"   - text-generation-webui\")\n",
    "        print(f\"   - LM Studio\")\n",
    "        print(f\"   - Any GGUF-compatible inference engine\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not fully validate file: {e}\")\n",
    "else:\n",
    "    print(\"❌ Final GGUF file not found\")\n",
    "\n",
    "print(f\"\\n🎯 CONVERSION COMPLETE!\")\n",
    "print(f\"The specialized Gemma 2B NIRF lookup model is now ready in GGUF format.\")\n",
    "\n",
    "# Optional: Show how to use the model\n",
    "print(f\"\\n💡 Usage example:\")\n",
    "print(f\"   llama.cpp: ./llama-cli -m {gguf_file} -p 'Your prompt here'\")\n",
    "print(f\"   Python: llama = Llama(model_path='{gguf_file}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NIRF model from: ./gguf_output/gemma2b-nirf-lookup-f16.gguf\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "NIRF Ranking Model Inference Script\n",
    "Uses the converted GGUF model to answer NIRF ranking questions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "\n",
    "class NIRFRankingModel:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Initialize the NIRF ranking model\"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.llm = None\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the GGUF model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading NIRF model from: {self.model_path}\")\n",
    "            self.llm = Llama(\n",
    "                model_path=self.model_path,\n",
    "                n_ctx=2048,  # Context window\n",
    "                n_threads=4,  # Number of CPU threads\n",
    "                verbose=False\n",
    "            )\n",
    "            print(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_response(self, prompt, max_tokens=512, temperature=0.3):\n",
    "        \"\"\"Generate response for NIRF ranking questions\"\"\"\n",
    "        if not self.llm:\n",
    "            raise Exception(\"Model not loaded\")\n",
    "        \n",
    "        try:\n",
    "            response = self.llm(\n",
    "                prompt,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                repeat_penalty=1.1,\n",
    "                stop=[\"</s>\", \"\\n\\n\"]\n",
    "            )\n",
    "            return response['choices'][0]['text'].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def ask_nirf_question(self, question):\n",
    "        \"\"\"Ask a NIRF ranking related question\"\"\"\n",
    "        # Format the prompt for NIRF queries\n",
    "        prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(\"Generating answer...\")\n",
    "        \n",
    "        response = self.generate_response(prompt)\n",
    "        if response:\n",
    "            print(f\"Answer: {response}\")\n",
    "        else:\n",
    "            print(\"Failed to generate response\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "def main():\n",
    "    # Path to your converted GGUF model\n",
    "    model_path = \"./gguf_output/gemma2b-nirf-lookup-f16.gguf\"\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        print(\"Please ensure the GGUF conversion was completed successfully.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the model\n",
    "    try:\n",
    "        nirf_model = NIRFRankingModel(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Sample NIRF ranking questions\n",
    "    sample_questions = [\n",
    "        \"What is the NIRF ranking of IIT Delhi in 2024?\",\n",
    "        \"Which university ranked first in the NIRF Engineering category in 2023?\",\n",
    "        \"How are NIRF rankings calculated?\",\n",
    "        \"What are the key parameters used in NIRF ranking methodology?\",\n",
    "        \"Which are the top 5 universities in NIRF Overall ranking 2024?\",\n",
    "        \"What is the difference between NIRF and other international rankings?\",\n",
    "        \"How often are NIRF rankings updated?\",\n",
    "        \"Which institution has consistently performed well in NIRF rankings?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== NIRF Ranking Model Inference Test ===\\n\")\n",
    "    \n",
    "    # Test with sample questions\n",
    "    for i, question in enumerate(sample_questions[:1], 1):  # Test first 3 questions\n",
    "        print(f\"\\n--- Test {i} ---\")\n",
    "        nirf_model.ask_nirf_question(question)\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"\\n=== Interactive Mode ===\")\n",
    "    print(\"Ask your NIRF ranking questions (type 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_question = input(\"\\nYour question: \").strip()\n",
    "            if user_question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            \n",
    "            if user_question:\n",
    "                nirf_model.ask_nirf_question(user_question)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hugging Face GGUF Upload Script ===\n",
      "\n",
      "🔑 Authenticating with Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722509605083438bb6a827de97096195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using existing HF token\n",
      "📁 File: ./gguf_output/gemma2b-nirf-lookup-f16.gguf\n",
      "📊 Size: 4.88 GB\n",
      "🎯 Target repo: coderop12/gemma2b-nirf-lookup-gguf\n",
      "✅ Repository created/verified: coderop12/gemma2b-nirf-lookup-gguf\n",
      "🔄 Uploading gemma2b-nirf-lookup-f16.gguf...\n",
      "This may take a while for large files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea543bba674c0dae140d8fe805ea99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769dccd25a824d89a13f2fbb1b4189d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa52f6cecf44189b126506b9f0f3688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...output/gemma2b-nirf-lookup-f16.gguf:   0%|          | 25.1MB / 5.24GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:586\u001b[0m, in \u001b[0;36m_upload_xet_files\u001b[0;34m(additions, repo_type, repo_id, headers, endpoint, revision, create_pr)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths_ops) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 586\u001b[0m     \u001b[43mupload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaths_ops\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccess_token_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bytes_ops) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 283\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. Or set HF_TOKEN in the script\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 283\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 258\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m uploader\u001b[38;5;241m.\u001b[39mauthenticate(HF_TOKEN)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Upload model\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[43muploader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_gguf_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_GGUF_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREPO_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoderop12/gemma2b-nirf-lookup-2025\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True if you want private repo\u001b[39;49;00m\n\u001b[1;32m    263\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎉 SUCCESS!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 183\u001b[0m, in \u001b[0;36mHuggingFaceUploader.upload_gguf_model\u001b[0;34m(self, local_file_path, repo_id, original_model, private, description)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Uploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may take a while for large files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m \u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUpload \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m GGUF model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Successfully uploaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Create and upload model card\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1669\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4710\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4702\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4703\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4704\u001b[0m )\n\u001b[1;32m   4705\u001b[0m operation \u001b[38;5;241m=\u001b[39m CommitOperationAdd(\n\u001b[1;32m   4706\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39mpath_or_fileobj,\n\u001b[1;32m   4707\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39mpath_in_repo,\n\u001b[1;32m   4708\u001b[0m )\n\u001b[0;32m-> 4710\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4713\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4720\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4723\u001b[0m     revision \u001b[38;5;241m=\u001b[39m quote(_parse_revision_from_pr_url(commit_info\u001b[38;5;241m.\u001b[39mpr_url), safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1669\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4239\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4236\u001b[0m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[1;32m   4237\u001b[0m _warn_on_overwriting_operations(operations)\n\u001b[0;32m-> 4239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[1;32m   4245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[1;32m   4248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4250\u001b[0m files_to_copy \u001b[38;5;241m=\u001b[39m _fetch_files_to_copy(\n\u001b[1;32m   4251\u001b[0m     copies\u001b[38;5;241m=\u001b[39mcopies,\n\u001b[1;32m   4252\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4256\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m   4257\u001b[0m )\n\u001b[1;32m   4258\u001b[0m \u001b[38;5;66;03m# Remove no-op operations (files that have not changed)\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4523\u001b[0m, in \u001b[0;36mHfApi.preupload_lfs_files\u001b[0;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[1;32m   4521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_buffered_io_data \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[1;32m   4522\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading files using Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4523\u001b[0m     \u001b[43m_upload_xet_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupload_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [arg-type]\u001b[39;00m\n\u001b[1;32m   4524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_enabled \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:586\u001b[0m, in \u001b[0;36m_upload_xet_files\u001b[0;34m(additions, repo_type, repo_id, headers, endpoint, revision, create_pr)\u001b[0m\n\u001b[1;32m    583\u001b[0m paths_ops \u001b[38;5;241m=\u001b[39m [op \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m _chunk \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op\u001b[38;5;241m.\u001b[39mpath_or_fileobj, (\u001b[38;5;28mstr\u001b[39m, Path))]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths_ops) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 586\u001b[0m     \u001b[43mupload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaths_ops\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccess_token_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bytes_ops) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    595\u001b[0m     upload_bytes(\n\u001b[1;32m    596\u001b[0m         [op\u001b[38;5;241m.\u001b[39mpath_or_fileobj \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m bytes_ops],\n\u001b[1;32m    597\u001b[0m         xet_endpoint,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m         repo_type,\n\u001b[1;32m    602\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to upload GGUF model to Hugging Face Hub\n",
    "Handles large file uploads and creates proper model repository\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from huggingface_hub import HfApi, login, create_repo, upload_file\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "class HuggingFaceUploader:\n",
    "    def __init__(self):\n",
    "        self.api = HfApi()\n",
    "        self.token = None\n",
    "        \n",
    "    def authenticate(self, token=None):\n",
    "        \"\"\"Authenticate with Hugging Face\"\"\"\n",
    "        if token:\n",
    "            login(token=token)\n",
    "            self.token = token\n",
    "            print(\"✅ Authenticated with provided token\")\n",
    "        else:\n",
    "            try:\n",
    "                # Try to use existing token\n",
    "                login()\n",
    "                print(\"✅ Using existing HF token\")\n",
    "            except Exception as e:\n",
    "                print(\"❌ Authentication failed\")\n",
    "                print(\"Please provide your HF token or run 'huggingface-cli login' first\")\n",
    "                raise e\n",
    "    \n",
    "    def create_model_card(self, repo_id, original_model, model_size, description=None):\n",
    "        \"\"\"Create a model card for the GGUF model\"\"\"\n",
    "        \n",
    "        # YAML frontmatter with proper metadata\n",
    "        yaml_header = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: {original_model}\n",
    "tags:\n",
    "- gguf\n",
    "- quantized\n",
    "- gemma\n",
    "- nirf\n",
    "- education\n",
    "- ranking\n",
    "- indian-universities\n",
    "- text-generation\n",
    "library_name: gguf\n",
    "model_name: {repo_id.split('/')[-1]}\n",
    "inference: false\n",
    "model_creator: {repo_id.split('/')[0]}\n",
    "model_type: gemma\n",
    "quantization: f16\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: text-generation\n",
    "widget:\n",
    "- text: \"What is NIRF ranking methodology?\"\n",
    "  example_title: \"NIRF Methodology\"\n",
    "- text: \"Which are the top engineering colleges in NIRF 2024?\"\n",
    "  example_title: \"Top Engineering Colleges\"\n",
    "- text: \"How are universities ranked in India?\"\n",
    "  example_title: \"University Rankings\"\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        default_description = f\"\"\"# {repo_id.split('/')[-1]}\n",
    "\n",
    "This is a GGUF conversion of [{original_model}](https://huggingface.co/{original_model}).\n",
    "\n",
    "## Model Details\n",
    "- **Original Model**: {original_model}\n",
    "- **Format**: GGUF (F16 precision)\n",
    "- **File Size**: ~{model_size:.1f} GB\n",
    "- **Architecture**: Gemma 2B\n",
    "- **Specialization**: NIRF (National Institutional Ranking Framework) lookup and ranking queries\n",
    "\n",
    "## Usage\n",
    "\n",
    "### With llama.cpp\n",
    "```bash\n",
    "./llama-cli -m {repo_id.split('/')[-1]}.gguf -p \"What is the NIRF ranking methodology?\"\n",
    "```\n",
    "\n",
    "### With Python (llama-cpp-python)\n",
    "```python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"{repo_id.split('/')[-1]}.gguf\")\n",
    "response = llm(\"What are the top NIRF ranked engineering colleges?\")\n",
    "print(response['choices'][0]['text'])\n",
    "```\n",
    "\n",
    "### With Ollama\n",
    "```bash\n",
    "# First, create a Modelfile\n",
    "echo 'FROM ./{repo_id.split('/')[-1]}.gguf' > Modelfile\n",
    "ollama create {repo_id.split('/')[-1]} -f Modelfile\n",
    "ollama run {repo_id.split('/')[-1]} \"Explain NIRF ranking parameters\"\n",
    "```\n",
    "\n",
    "## Model Capabilities\n",
    "This model is specifically fine-tuned for:\n",
    "- NIRF ranking information and queries\n",
    "- Indian higher education institutional data\n",
    "- University and college ranking explanations\n",
    "- Educational policy and framework questions\n",
    "\n",
    "## Technical Details\n",
    "- **Quantization**: F16 (16-bit floating point)\n",
    "- **Context Length**: 2048 tokens\n",
    "- **License**: Follow original model license terms\n",
    "- **Converted using**: llama.cpp conversion tools\n",
    "\n",
    "## Original Model License\n",
    "Please refer to the original model repository for license information.\n",
    "\"\"\"\n",
    "        \n",
    "        # Combine YAML header with description\n",
    "        if description:\n",
    "            model_card_content = yaml_header + description\n",
    "        else:\n",
    "            model_card_content = yaml_header + default_description\n",
    "        \n",
    "        return model_card_content\n",
    "    \n",
    "    def create_repository(self, repo_id, private=False):\n",
    "        \"\"\"Create HF repository\"\"\"\n",
    "        try:\n",
    "            create_repo(\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"model\",\n",
    "                private=private,\n",
    "                exist_ok=True\n",
    "            )\n",
    "            print(f\"✅ Repository created/verified: {repo_id}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to create repository: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def upload_gguf_model(self, \n",
    "                         local_file_path, \n",
    "                         repo_id, \n",
    "                         original_model=\"coderop12/gemma2b-nirf-lookup-2025\",\n",
    "                         private=False,\n",
    "                         description=None):\n",
    "        \"\"\"\n",
    "        Upload GGUF model to Hugging Face\n",
    "        \n",
    "        Args:\n",
    "            local_file_path: Path to your GGUF file\n",
    "            repo_id: HF repo ID (username/model-name)\n",
    "            original_model: Original model this was converted from\n",
    "            private: Whether to make repo private\n",
    "            description: Custom model card description\n",
    "        \"\"\"\n",
    "        \n",
    "        if not os.path.exists(local_file_path):\n",
    "            print(f\"❌ File not found: {local_file_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Get file info\n",
    "        file_size_gb = os.path.getsize(local_file_path) / (1024**3)\n",
    "        file_name = os.path.basename(local_file_path)\n",
    "        \n",
    "        print(f\"📁 File: {local_file_path}\")\n",
    "        print(f\"📊 Size: {file_size_gb:.2f} GB\")\n",
    "        print(f\"🎯 Target repo: {repo_id}\")\n",
    "        \n",
    "        # Create repository\n",
    "        if not self.create_repository(repo_id, private):\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Upload the GGUF file\n",
    "            print(f\"🔄 Uploading {file_name}...\")\n",
    "            print(\"This may take a while for large files...\")\n",
    "            \n",
    "            upload_file(\n",
    "                path_or_fileobj=local_file_path,\n",
    "                path_in_repo=file_name,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=f\"Upload {file_name} GGUF model\"\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Successfully uploaded {file_name}\")\n",
    "            \n",
    "            # Create and upload model card\n",
    "            print(\"📝 Creating model card...\")\n",
    "            model_card = self.create_model_card(repo_id, original_model, file_size_gb, description)\n",
    "            \n",
    "            # Save model card to temporary file and upload\n",
    "            readme_path = \"README.md\"\n",
    "            with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(model_card)\n",
    "            \n",
    "            upload_file(\n",
    "                path_or_fileobj=readme_path,\n",
    "                path_in_repo=\"README.md\",\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"model\",\n",
    "                commit_message=\"Add model card\"\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            os.remove(readme_path)\n",
    "            \n",
    "            print(f\"✅ Model card uploaded\")\n",
    "            print(f\"🎉 Upload complete! View at: https://huggingface.co/{repo_id}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Upload failed: {e}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main upload script\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    LOCAL_GGUF_PATH = \"./gguf_output/gemma2b-nirf-lookup-f16.gguf\"\n",
    "    \n",
    "    # You need to set these values\n",
    "    HF_USERNAME = \"coderop12\"  # Replace with your HF username\n",
    "    MODEL_NAME = \"gemma2b-nirf-lookup-gguf\"  # Choose your model name\n",
    "    REPO_ID = f\"{HF_USERNAME}/{MODEL_NAME}\"\n",
    "    \n",
    "    # Optional: Use your HF token directly\n",
    "    HF_TOKEN = None  # Replace with your token or leave None to use saved token\n",
    "    \n",
    "    print(\"=== Hugging Face GGUF Upload Script ===\\n\")\n",
    "    \n",
    "    # Validate inputs\n",
    "    if HF_USERNAME == \"YOUR_USERNAME\":\n",
    "        print(\"❌ Please set your HF_USERNAME in the script\")\n",
    "        print(\"Edit the script and replace 'YOUR_USERNAME' with your Hugging Face username\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(LOCAL_GGUF_PATH):\n",
    "        print(f\"❌ GGUF file not found: {LOCAL_GGUF_PATH}\")\n",
    "        print(\"Make sure your GGUF conversion completed successfully\")\n",
    "        return\n",
    "    \n",
    "    # Initialize uploader\n",
    "    uploader = HuggingFaceUploader()\n",
    "    \n",
    "    try:\n",
    "        # Authenticate\n",
    "        print(\"🔑 Authenticating with Hugging Face...\")\n",
    "        uploader.authenticate(HF_TOKEN)\n",
    "        \n",
    "        # Upload model\n",
    "        success = uploader.upload_gguf_model(\n",
    "            local_file_path=LOCAL_GGUF_PATH,\n",
    "            repo_id=REPO_ID,\n",
    "            original_model=\"coderop12/gemma2b-nirf-lookup-2025\",\n",
    "            private=False,  # Set to True if you want private repo\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\n🎉 SUCCESS!\")\n",
    "            print(f\"Your GGUF model is now available at:\")\n",
    "            print(f\"https://huggingface.co/{REPO_ID}\")\n",
    "            print(f\"\\nUsers can download it with:\")\n",
    "            print(f\"huggingface-cli download {REPO_ID} {os.path.basename(LOCAL_GGUF_PATH)}\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Upload failed. Check the errors above.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Script failed: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Make sure you have a Hugging Face account\")\n",
    "        print(\"2. Get your access token from https://huggingface.co/settings/tokens\")\n",
    "        print(\"3. Run: huggingface-cli login\")\n",
    "        print(\"4. Or set HF_TOKEN in the script\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "path = hf_hub_download(\"coderop12/gemma2b-nirf-lookup-gguf\", \"gemma2b-nirf-lookup-f16.gguf\")\n",
    "llm = Llama(\n",
    "    model_path=path,\n",
    "    n_ctx=1024,\n",
    "    n_threads=4,\n",
    "    n_batch=128,\n",
    "    n_gpu_layers=0,\n",
    "    use_mmap=True,\n",
    "    use_mlock=False,\n",
    "    verbose=False,\n",
    ")\n",
    "print(\"Loaded!\")\n",
    "\n",
    "out = llm(\"Question: Which are the top 5 universities in NIRF Overall ranking 2024?\\n\\nAnswer:\",\n",
    "          max_tokens=256, temperature=0.3, top_p=0.9, repeat_penalty=1.1, stop=[\"</s>\", \"\\n\\n\"])\n",
    "print(out[\"choices\"][0][\"text\"].strip())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
