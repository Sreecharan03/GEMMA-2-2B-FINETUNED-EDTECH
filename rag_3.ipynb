{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è  Downloading from HF: coderop12/gemma2b-nirf-lookup-gguf/gemma2b-nirf-lookup-f16.gguf\n",
      "‚úÖ Model ready at: /teamspace/studios/this_studio/.cache/huggingface/hub/models--coderop12--gemma2b-nirf-lookup-gguf/snapshots/113a395c55a965971c1f6a5ddb890245ee495f3b/gemma2b-nirf-lookup-f16.gguf\n",
      "üîß Loading GGUF: /teamspace/studios/this_studio/.cache/huggingface/hub/models--coderop12--gemma2b-nirf-lookup-gguf/snapshots/113a395c55a965971c1f6a5ddb890245ee495f3b/gemma2b-nirf-lookup-f16.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Load GGUF from Hugging Face and run NIRF Q&A (CLI).\n",
    "Repo: coderop12/gemma2b-nirf-lookup-gguf\n",
    "File: gemma2b-nirf-lookup-f16.gguf\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Optional\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "HF_REPO_ID = \"coderop12/gemma2b-nirf-lookup-gguf\"\n",
    "HF_FILENAME = \"gemma2b-nirf-lookup-f16.gguf\"\n",
    "# If the repo is private, set HF_TOKEN in your env; otherwise leave None\n",
    "HF_TOKEN: Optional[str] = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "# llama.cpp runtime settings (tweak as you like)\n",
    "N_CTX = 2048\n",
    "N_THREADS = int(os.environ.get(\"N_THREADS\", \"4\"))\n",
    "N_GPU_LAYERS = int(os.environ.get(\"N_GPU_LAYERS\", \"0\"))  # set >0 if you have GPU offload\n",
    "VERBOSE = False\n",
    "\n",
    "def download_model() -> str:\n",
    "    \"\"\"Download the GGUF file from Hugging Face (cached locally by HF).\"\"\"\n",
    "    print(f\"‚¨áÔ∏è  Downloading from HF: {HF_REPO_ID}/{HF_FILENAME}\")\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=HF_REPO_ID,\n",
    "        filename=HF_FILENAME,\n",
    "        local_dir=None,         # use HF cache dir\n",
    "        token=HF_TOKEN,         # None if public\n",
    "        force_download=False,\n",
    "        resume_download=True\n",
    "    )\n",
    "    print(f\"‚úÖ Model ready at: {local_path}\")\n",
    "    return local_path\n",
    "\n",
    "class NIRFRankingModel:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.llm = None\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        print(f\"üîß Loading GGUF: {self.model_path}\")\n",
    "        self.llm = Llama(\n",
    "            model_path=self.model_path,\n",
    "            n_ctx=N_CTX,\n",
    "            n_threads=N_THREADS,\n",
    "            n_gpu_layers=N_GPU_LAYERS,\n",
    "            verbose=VERBOSE\n",
    "        )\n",
    "        print(\"‚úÖ Model loaded\")\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.3) -> str:\n",
    "        out = self.llm(\n",
    "            prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            repeat_penalty=1.1,\n",
    "            stop=[\"</s>\", \"\\n\\n\"]\n",
    "        )\n",
    "        return (out[\"choices\"][0][\"text\"] or \"\").strip()\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        prompt = f\"Question: {question}\\n\\nAnswer:\"\n",
    "        return self.generate(prompt)\n",
    "\n",
    "def main():\n",
    "    model_path = download_model()\n",
    "    nirf = NIRFRankingModel(model_path)\n",
    "\n",
    "    print(\"=== NIRF GGUF Inference (from Hugging Face) ===\")\n",
    "    samples = [\n",
    "        \"What is the NIRF ranking of IIT Delhi in 2024?\",\n",
    "        \"Which are the top 5 universities in NIRF Overall ranking 2024?\",\n",
    "    ]\n",
    "    for i, q in enumerate(samples, 1):\n",
    "        print(f\"\\n--- Test {i} ---\")\n",
    "        print(\"Q:\", q)\n",
    "        a = nirf.ask(q)\n",
    "        print(\"A:\", a)\n",
    "\n",
    "    print(\"\\n=== Interactive Mode ===\")\n",
    "    print(\"Type 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            q = input(\"\\nYour question: \").strip()\n",
    "            if q.lower() in {\"quit\", \"exit\", \"q\"}:\n",
    "                break\n",
    "            print(\"A:\", nirf.ask(q))\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            29Gi       3.8Gi       461Mi       2.1Mi        25Gi        25Gi\n",
      "Swap:           15Gi       256Ki        15Gi\n"
     ]
    }
   ],
   "source": [
    "!free -h\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
